# Extracted Text for HOHWY_2020_New_Directions_in_Predictive_Processing_Framework_for_Neurodiversity_and_Critique_of_Standardization.pdf.pdf

DOCUMENT SUMMARY

This review by Jakob Hohwy, "New directions in predictive processing," provides a detailed 
formal framework for understanding the brain as a context-sensitive, experience-dependent 
inference engine. This paper offers powerful scientific validation for Enlitens' mission by 
formalizing how individual prior experiences shape perception and cognition, directly 
challenging the premise of standardized, context-free assessments. It reframes 
neurodivergence, such as autism, and mental health conditions as logical, alternative modes of 
predictive modeling based on unique priors, rather than deficits, which strongly supports a 
clinical interview model focused on understanding an individual's unique inferential landscape.

FILENAME

HOHWY_2020_New_Directions_in_Predictive_Processing_Framework_for_Neurodiversity_and
_Critique_of_Standardization.pdf

METADATA

● Primary Category: RESEARCH
● Document Type: research_article
● Relevance: Core
● Key Topics: predictive_processing, bayesian_inference, neurodiversity_paradigm, 

context_sensitivity, cognitive_penetrability, autism, mental_health, 
computational_neuropsychiatry

● Tags: #PredictiveProcessing, #BayesianBrain, #Neurodiversity, #Autism, 

#ContextSensitivity, #ClinicalInterview, #AntiStandardization, #CognitivePenetrability, 
#MentalHealth, #4ECognition, #Priors

CRITICAL QUOTES FOR ENLITENS

"In predictive coding, a given system (such as the human brain) harbours an internal model of 
the causes of its sensory input. These are hidden causes in the sense that the system does not 
have direct access to them but must infer them on the basis of its sensory input and prior 
knowledge."

"Through repeated steps of perceptual inference across the hierarchy, the internal model comes
to recapitulate the causal structure and dynamics of the environment around the system."

"PP is thus focused on systems that minimize prediction error, and thereby approximate 
Bayesian inference (where systems that can minimize error in a changing, volatile world, 
approximate hierarchical, empirical Bayesian inference)."

"Systems that can minimise error only need to access their model and the sensory input. 
Therefore, PP systems appear capable of unsupervised learning and inference, that is, they do 
not have to rely on externally labelled training data."

"On the one hand, there is always some degree of cognitive penetrability in perception, since 
(setting aside limit cases) the prior is always given some weighting. On the other hand, not all 
beliefs are guaranteed weighting since perceptual inference is subject to precision-weighting 
extracted in prior learning as well as model selection and complexity considerations."

"PP suggests, rather, that attention is needed because we must engage in unsupervised 
learning and inference in a changeable and volatile world where we need to continuously re-
assess the way we balance prior belief with incoming information."

"It follows from the complete class theorems that, indeed, any inference, even apparently sub-
optimal inference, can be framed as optimal and that its apparent sub-optimality affords an 
opportunity to explore the sorts of prior beliefs that render it optimal (Parr et al., 2018). This can 
then be used to phenotype mental disorder, for example."

KEY STATISTICS & EVIDENCE

This document is a theoretical review and does not present new statistical findings. However, it 
provides a comprehensive list (Table S1) of empirical and theoretical work that serves as 
evidence for the predictive processing framework across various domains highly relevant to 
Enlitens.

Evidence from Cited Research on Neurodiversity and Mental Health:

The supplementary table (Table S1) provides an extensive bibliography of research applying the
Predictive Processing framework to areas central to Enlitens' mission, demonstrating a robust 
scientific basis for these applications:

● Autism: Research is cited on autism in relation to perceptual inference, active inference,

social cognition, and the self. Specific papers listed include:

○ "Precise minds in uncertain worlds: Predictive coding in autism" (Van de Cruys et

al., 2014)

○ "An aberrant precision account of autism" (Lawson et al., 2014)
○ "Bayesian Approaches to Autism: Towards Volatility, Action, and Behavior" 

(Palmer et al., 2017)

○ "The felt presence of other minds: Predictive processing, counterfactual 

predictions, and mentalising in autism" (Palmer et al., 2015)

○ "Modelling Me, Modelling You: the Autistic Self" (Perrykkad and Hohwy, 2019)

● Mental Health: Research is cited on applying the PP framework to:

○ Delusion: (Frith, 1992; Davies and Coltheart, 2000; Hohwy, 2004; Bortolotti and 

Miyazono, 2015; Williams, 2018a; Corlett, 2019)
○ Hallucinations: (Corlett et al., 2019; Wilkinson, 2014)
○ Psychosis in general: (Sterzer et al., 2018)
○ Depression, anxiety, and stress: (Badcock et al., 2017; Peters et al., 2017)
○ Trauma: (Wilkinson et al., 2017)
○ Eating disorders: (Gadsby and Hohwy, In print.)

● Interoception & Brain-Body Coupling: Research is cited connecting PP to the sense 
of the body's internal state, a key area for understanding trauma, anxiety, and emotion. 
Specific papers listed include:

○ "Allostasis, interoception, and the free energy principle: Feeling our way forward" 

(Corcoran and Hohwy, 2018)

○ "Interoceptive inference, emotion, and the embodied self" (Seth, 2013)
○ "The neurobiology of interoception in health and disease" (Quadt et al., 2018)

THEORETICAL FRAMEWORKS

Predictive Coding and Approximate Inference

This framework posits that the brain operates an internal, generative model of the world to 
predict its sensory inputs. The brain does not have direct access to the "hidden causes" of 
sensation and must infer them based on sensory input and prior knowledge.

● Prediction Error: The core mechanism involves a comparison between top-down 

predictions and bottom-up sensory signals. Any mismatch generates a "prediction error" 
signal that ascends through the brain's hierarchy.

● Model Updating: This prediction error is then used to update the internal model, leading

to new, more accurate predictions.

● Precision Weighting (Context Sensitivity): The influence of prediction error is not 

absolute; it is weighted by "precision" (the inverse of variance). The system balances the
precision of its prior knowledge (
πP) against the precision of the current sensory input (πL). The formula
 new prediction = old prediction + ((πι. / (πρ + πι.)) * 
prediction error) shows that the learning rate increases with the precision of new 
evidence and decreases with the precision of prior knowledge. This allows the system to
"largely ignore imprecise new evidence" and trust its existing model when appropriate.

● Precision Optimization: The system can also infer and predict the expected precision 

of signals, allowing it to create a variable learning rate sensitive to context. For example, 
the weight on visual prediction error is decreased when navigating a familiar room 
without glasses. This makes the system highly context-dependent and able to deal with 
noise, ambiguity, and volatility.

● Approximating Bayesian Inference: The central insight is that by continuously 

minimizing prediction error, the system effectively approximates Bayesian inference 
without needing to perform exact, computationally expensive calculations. This makes 
the theory biologically plausible.

The Predictive Processing (PP) Toolbox

The full PP framework goes beyond basic predictive coding to include a set of processes a 
system uses to keep prediction error low.

● Active Inference: Action is a key tool for minimizing prediction error. Instead of 

changing its internal model, the system can act on the world to make its sensory input 
match its predictions. For example, the prediction that a cup is in your hand generates a 
prediction error that is minimized by the action of reaching for and grasping the cup. 

Action is used both for epistemic value (to reduce uncertainty) and for utility (to obtain 
desired states).

● Self-Fulfilling Prophesying: The system tends to occupy states it expects to occupy by
selectively sampling those states through action. This explains why an organism doesn't 
just seek a dark room to minimize error; a human, for example, expects to be in varied, 
stimulating environments, and so infers policies (actions) that lead to those states.
● Model Selection and Complexity Reduction: A system trying to minimize error long-
term will also simplify its internal model to avoid overfitting, adhering to a form of 
Occam's razor. It will engage in model selection, choosing the models that most 
efficiently minimize prediction error.

Pluralistic PP vs. The Free Energy Principle (FEP)

There are two main ways to interpret the PP framework's scope.

● Pluralistic PP: This view, favored by Clark (2013, 2016), sees PP as a collection of 

predictive processes that a system might use, potentially alongside non-PP processes. It
is a matter of empirical discovery to find where and how PP operates.

● Unifying FEP: This more reductive view posits that all PP processes are the result of a 
single underlying principle, the Free Energy Principle (FEP), which states that any self-
organizing system must minimize its free energy to exist. Free energy is mathematically 
linked to the long-term average of prediction error. FEP is seen as a principle from which
process theories like predictive coding can be derived.

Cognitive Penetrability (The Influence of Belief on Perception)

The PP framework reinvigorates the debate about whether high-level beliefs can influence low-
level perception.

● A Middle Ground: PP provides a formal way to mediate this debate. On one hand, 

some degree of cognitive penetration is always happening, as the brain's "prior" is 
always given some weight in perception. On the other hand, this influence is not 
absolute; it is constrained by precision-weighting based on prior learning and the 
reliability of sensory input.

● Explaining Robustness and Malleability: Strong cases of cognitive penetration are 
possible, but only when the context creates high uncertainty in the sensory signal, 
opening a window for a high-level belief (prior) to dominate the inference. This explains 
why perception can sometimes be influenced by belief but often remains robust against 
it. This directly supports a clinical interview approach, which seeks to understand the 
client's internal model (priors) to make sense of their perceptual experiences.

PRACTICAL APPLICATIONS

Computational Neuropsychiatry: A Strengths-Based Model of "Disorder"

The PP framework offers a powerful, non-pathologizing approach to understanding mental 
health and neurodivergence, which is directly applicable to Enlitens' clinical methodology.

● Reframing "Sub-Optimal" Inference: An objection often raised against Bayesian 

models is that they can be fit to any behavior, making them unfalsifiable. However, this is
turned into a clinical strength within computational neuropsychiatry.

● Phenotyping via Priors: The "complete class theorems" show that "any inference, even

apparently sub-optimal inference, can be framed as optimal". The apparent sub-
optimality provides an opportunity to investigate and identify "the sorts of prior beliefs 
that render it optimal".

● Application: "This can then be used to phenotype mental disorder, for example". This is
a direct blueprint for the Enlitens Interview: instead of labeling a person's responses as 
disordered or deficient according to a standardized norm, the goal is to understand the 
underlying generative model and prior beliefs that make that person's unique way of 
processing the world perfectly logical and optimal
 for them. This shifts the focus from deficit-finding to sense-making.

